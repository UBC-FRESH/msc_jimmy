{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Forest estate modelling input data prep \n",
    "\n",
    "This notebook was originally designed and implemented by Gregory Paradis (gregory.paradis@ubc.ca). For the project use, the model has been fine tunned by Jimmy Ke (jimmy49594823@gmail.com)\n",
    "\n",
    "This notebook implements a data processing pipeline to prepare input datasets for the landscape-level forest estate modelling (e.g., ws3, SpaDES, Patchworks, etc.) for timber supply areas (TSAs) in British Columbia (BC).\n",
    "\n",
    "This notebook imports various publicly-available vegetation resource inventory (VRI) datasets, extracts the RIA landbase subset, stratifies features in a way that can work for the harvesting and carbon analyses we are planning, and builds VDYP and TIPSY yield curves.\n",
    "\n",
    "The source datasets are all freely available, but need to be downloaded and unpacked to paths that match the paths specified further down in the notebook (or modify the path parameter values). We list links to datasets used below.\n",
    "\n",
    "This algorithms and input data files in this project are custom built to compile input data for computational experiments on a cluster of five TSAs in the north-east corner of BC (i.e., TSAs 08, 16, 24, 40, 41). However, our intention while building this prototype was to make this tool generic and modular enough to be re-usable (to a large extent) for any (or all!) TSAs in BC. In other words, copying this project and adapting it to work with other TSAs is likely a _much_ better idea than starting from scratch. Later, we our intent is to extend this work until it includes reasonable default turn-key behaviour for _all_ TSAs in BC---this should make greatly reduce the cost and difficulty of compiling inventory and yield curve datasets to be used as input for new forest estate models of public TSAs in BC. \n",
    "\n",
    "## Datasets\n",
    "\n",
    "Links to source datasets used in this notebook.\n",
    "\n",
    "[FADM - Timber Supply Area (TSA)](https://catalogue.data.gov.bc.ca/dataset/8daa29da-d7f4-401c-83ae-d962e3a28980)\n",
    "\n",
    "[VRI - 2019 - Forest Vegetation Composite Rank 1 Layer (R1)](https://catalogue.data.gov.bc.ca/dataset/2ebb35d8-c82f-4a17-9c96-612ac3532d55)\n",
    "\n",
    "[VRI - 2019 - Variable Density Yield Projection 7 (VDYP7) Input Polygon](https://catalogue.data.gov.bc.ca/dataset/57513aaa-c0a6-41a9-b2a8-b980b1604ee6)\n",
    "\n",
    "[Site Productivity - Site Index by Tree Species](https://catalogue.data.gov.bc.ca/dataset/04ad45c3-0fdc-4506-bdb4-252c45a63459)\n",
    "\n",
    "[Generalized Forest Cover Ownership](https://catalogue.data.gov.bc.ca/dataset/5fc4e8ce-dd1d-44fd-af17-e0789cf65e4ehttps://catalogue.data.gov.bc.ca/dataset/5fc4e8ce-dd1d-44fd-af17-e0789cf65e4e)\n",
    "\n",
    "See also [VRI code lists](https://www.for.gov.bc.ca/hfp/publications/00026/fs708-14-appendix_d.htm) for some metadata that can help you decode and understand some of the datasets we use.\n",
    "\n",
    "## Software dependencies\n",
    "\n",
    "Python packages imported below must be installed. We also use a [patched branch](https://github.com/gparadis/ArcRasterRescue/tree/bug-fix_transform) of the ArcRasterRescue package to export species-wise site productivity raster data layers from the proprietary ESRI File Geodatabase file.\n",
    "\n",
    "Also check this out for BC raster data:\n",
    "\n",
    "https://hectaresbc.ca/app/habc/HaBC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the correct branch of the ArcRasterRescue repository from GitHub, and compile the executable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install missing package dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas geopandas numpy seaborn rasterio ipyparallel swifter distance pyarrow openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install git+https://github.com/eth-cscs/ipcluster_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import datatable\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from shapely.ops import unary_union, Polygon\n",
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial\n",
    "import csv\n",
    "from scipy.optimize import curve_fit\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.mask import mask\n",
    "from rasterio.io import MemoryFile\n",
    "#from geocube.api.core import make_geocube\n",
    "import subprocess\n",
    "import shlex\n",
    "from rasterio.plot import show, show_hist\n",
    "import warnings\n",
    "#from pympler import asizeof\n",
    "import ipcmagic\n",
    "import ipyparallel as ipp\n",
    "#import mapply\n",
    "import swifter\n",
    "import fiona\n",
    "import affine\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "import operator\n",
    "import distance\n",
    "from scipy.optimize import curve_fit as _curve_fit\n",
    "from functools import partial, wraps\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global notebook parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300) # bump this parameter up if you want to see more table rows\n",
    "pd.set_option('display.max_columns', 30) # bump this parameter up if you want to see more table rows\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "%config Completer.use_jedi = False # patch autocomplete bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch and set up `ipyparallel` cluster on the VM. We will deploy parallel processing jobs to this cluster at various stages in the process to speed up processing (take advantage of 16 cores available on the host)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ipcluster --help\n",
    "%ipcluster start -n 16\n",
    "rc = ipp.Client()\n",
    "lbview = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path strings and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_vclr1p_path = '../data/bc/vri/2019/VEG_COMP_LYR_R1_POLY.gdb'\n",
    "ria_stands_path = './data/veg_comp_lyr_r1_poly-ria.shp'\n",
    "tsa_boundaries_path = '../data/bc/tsa/FADM_TSA.gdb/'\n",
    "ria_maptiles_path = 'ria_maptiles.csv'\n",
    "vdyp_input_pandl_path = '../data/bc/vri/2019/VEG_COMP_VDYP7_INPUT_POLY_AND_LAYER_2019.gdb'\n",
    "\n",
    "site_prod_bc_gdb_path = './data/Site_Prod_BC.gdb/' # ESRI File Geodatabase containing 22 species-wise site productivity raster layers\n",
    "\n",
    "tsa_boundaries_feather_path = './data/tsa_boundaries.feather'\n",
    "ria_vri_vclr1p_checkpoint1_feather_path = './data/ria_vri_vclr1p_checkpoint1.feather'\n",
    "ria_vri_vclr1p_checkpoint2_feather_path = './data/ria_vri_vclr1p_checkpoint2.feather'\n",
    "ria_vri_vclr1p_checkpoint3_feather_path = './data/ria_vri_vclr1p_checkpoint3.feather'\n",
    "ria_vri_vclr1p_checkpoint4_feather_path = './data/ria_vri_vclr1p_checkpoint4.feather'\n",
    "ria_vri_vclr1p_checkpoint5_feather_path = './data/ria_vri_vclr1p_checkpoint5.feather'\n",
    "ria_vri_vclr1p_checkpoint6_feather_path = './data/ria_vri_vclr1p_checkpoint6.feather'\n",
    "ria_vri_vclr1p_checkpoint7_feather_path = './data/ria_vri_vclr1p_checkpoint7.feather'\n",
    "ria_vri_vclr1p_checkpoint8_feather_path = './data/ria_vri_vclr1p_checkpoint8.feather'\n",
    "vri_vclr1p_categorical_columns_path = './data/vri_vclr1p_categorical_columns'\n",
    "ria_vclr1p_feature_tif_path = './data/ria_vclr1p_feature_raster.tif'\n",
    "\n",
    "arc_raster_rescue_exe_path = 'ArcRasterRescue/build/arc_raster_rescue.exe'\n",
    "siteprod_gdb_path = '../data/Site_Prod_BC.gdb/'\n",
    "siteprod_tmpexport_tif_path_prefix = './data/site_prod_bc_'\n",
    "siteprod_tif_path = './data/siteprod.tif'\n",
    "\n",
    "vdyp_ply_feather_path = './data/vdyp_ply.feather'\n",
    "vdyp_lyr_feather_path = './data/vdyp_lyr.feather'\n",
    "vdyp_results_tsa_pickle_path_prefix = './data/vdyp_results-tsa'\n",
    "vdyp_results_pickle_path = './data/vdyp_results.pkl'\n",
    "vdyp_curves_smooth_tsa_feather_path_prefix = './data/vdyp_curves_smooth-tsa'\n",
    "vdyp_curves_smooth_feather_path = './data/vdyp_curves_smooth.feather'\n",
    "\n",
    "tipsy_params_path_prefix = './data/tipsy_params_tsa'\n",
    "\n",
    "#ria_tsas = ['08', '16', '24', '40', '41']\n",
    "ria_tsas = ['24']\n",
    "si_levels = ['L', 'M', 'H']\n",
    "\n",
    "raster_pxw = raster_pxh = 100\n",
    "\n",
    "tipsy_params_columns = [line.strip() for line in open('./data/tipsy_params_columns').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lists grouping species codes by genus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_spruce = ['S', 'SB', 'SE', 'SN', 'SS', 'SW', 'SX', 'SXE', 'SXL', 'SXW']\n",
    "species_pine = ['P', 'PA', 'PJ', 'PL', 'PLC', 'PLI', 'PM']\n",
    "species_fir = ['B', 'BA', 'BB', 'BG', 'BL', 'BM', 'BP']\n",
    "species_larch = ['L', 'LA', 'LS', 'LT', 'LW']\n",
    "species_cedar = ['C', 'CW']\n",
    "species_hemlock = ['HM', 'HWI', 'HW']\n",
    "species_douglasfir = ['F', 'FD', 'FDC', 'FDI']\n",
    "\n",
    "species_aspen = ['AC', 'ACB', 'ACT', 'AD', 'AT', 'AX']\n",
    "species_birch = ['E', 'EA', 'EB', 'EE', 'EP', 'EW', 'EXP']\n",
    "species_willow = ['W','WA', 'WB', 'WD', 'WP', 'WS']\n",
    "species_alder = ['D', 'DR']\n",
    "species_cherry = ['V']\n",
    "species_dogwood = ['GP']\n",
    "species_oak = ['Q']\n",
    "species_maple = ['M', 'MB', 'MV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and compile ArcRasterRescue package. We will use this later to extract raster data from proprietary ESRI File Geodatabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(arc_raster_rescue_exe_path):\n",
    "    !git clone https://github.com/gparadis/ArcRasterRescue.git\n",
    "    !cd ArcRasterRescue; git checkout bug-fix_transform\n",
    "    !mkdir ArcRasterRescue/build\n",
    "    !cd ArcRasterRescue/build; cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo ..; make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load TSA boundary data for the 5 TSAs in the RIA landbase (TSAs 08, 16, 24, 40, 41) and compile a single polygon defining RIA landbase extent. We simplify the boundary geometry for each TSA (preserving topology). Cache a copy of result to a feather file (can later be imported to speed up processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_tsa_boundaries_data = 0\n",
    "if import_tsa_boundaries_data:\n",
    "    tsa_boundaries = gpd.read_file(tsa_boundaries_path)\n",
    "    tsa_boundaries = tsa_boundaries[['TSA_NUMBER', 'geometry']].loc[tsa_boundaries.TSA_NUMBER.isin(ria_tsas)].dissolve(by='TSA_NUMBER')\n",
    "    tsa_boundaries['geometry'] = tsa_boundaries.geometry.simplify(tolerance=1000, preserve_topology=True)\n",
    "    tsa_boundaries.to_feather(tsa_boundaries_feather_path)\n",
    "else:\n",
    "    tsa_boundaries = gpd.read_feather(tsa_boundaries_feather_path)\n",
    "\n",
    "#tsa_extent_all = Polygon(unary_union(list(tsa_boundaries.geometry.values)).exterior) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load VRI features (_vegetation composite layer and R1 polygon_ dataset), masked to RIA landbase extent. Cache a copy of result to a feather file (can later be imported to speed up processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import_vri_vclr1p_data = 0 # set to False to use cached data \n",
    "if import_vri_vclr1p_data:\n",
    "    print('loading VRI data from source')\n",
    "    def load_vri_vclr1p(vri_vclr1p_path, tsa_code, tsa_mask, ignore_geometry=False):\n",
    "        import geopandas as gpd # local import required to play nice with ipp engines\n",
    "        result = gpd.read_file(vri_vclr1p_path, \n",
    "                               mask=tsa_mask,\n",
    "                               ignore1_geometry=ignore_geometry)\n",
    "        result['tsa_code'] = tsa_code\n",
    "        return result\n",
    "    tsa_gdfs = lbview.map_async(load_vri_vclr1p, \n",
    "                                [vri_vclr1p_path for tsa in ria_tsas],\n",
    "                                ria_tsas, \n",
    "                                [tsa_boundaries.loc[tsa].geometry for tsa in ria_tsas],\n",
    "                                ordered=True)\n",
    "    rc.wait_interactive()\n",
    "    ria_vri_vclr1p = pd.concat(tsa_gdfs, axis=0, ignore_index=True)\n",
    "    ria_vri_vclr1p.to_feather(ria_vri_vclr1p_checkpoint1_feather_path)\n",
    "else:\n",
    "    print('loading VRI data from checkpoint1 feather')\n",
    "    ria_vri_vclr1p = gpd.read_feather(ria_vri_vclr1p_checkpoint1_feather_path)\n",
    "\n",
    "f = ria_vri_vclr1p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract site productivity raster data from proprietary ESRI File Geodatabase, and export it to species-wise GeoTIFF layers, patch the missing CRS metadata, and stack everything into a single multi-band GeoTIFF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run ArcRasterRescue on siteprod ESRI File GeoDatabase raster dataset\n",
    "result = subprocess.run(shlex.split('%s %s' % (arc_raster_rescue_exe_path, siteprod_gdb_path)), capture_output=True)\n",
    "# map layer indices to layer species codes\n",
    "siteprod_layerspecies =  {int(i):vv[10:].upper() \n",
    "                          for i, vv in [v.strip().split(' ') \n",
    "                                        for v in result.stdout.decode().split('\\n')[1:] if v]}\n",
    "siteprod_specieslayer =  {vv[10:].upper():int(i) \n",
    "                          for i, vv in [v.strip().split(' ') \n",
    "                                        for v in result.stdout.decode().split('\\n')[1:] if v]}\n",
    "\n",
    "if not os.path.isfile(siteprod_tif_path):\n",
    "\n",
    "    # export species-wise raster layers to 22 GeoTIFFs\n",
    "    print('Extracting siteprod raster data from ESRI File Geodatabase...')\n",
    "    for i, species in siteprod_layerspecies.items():\n",
    "        print('... processing species', species)\n",
    "        args = '%s %s %i %s' % (arc_raster_rescue_exe_path, \n",
    "                                siteprod_gdb_path, \n",
    "                                i, \n",
    "                                '%s%s.tif' % (siteprod_tmpexport_tif_path_prefix, species))\n",
    "        subprocess.run(shlex.split(args))\n",
    "\n",
    "    # stack species-wise raster layers into a single multi-band GeoTIFF\n",
    "    file_list = sorted(glob.glob('%s*.tif' % siteprod_tmpexport_tif_path_prefix))\n",
    "    with rio.open(file_list[0]) as src: # patch with missing CRS metadata\n",
    "        meta = src.meta\n",
    "        meta.update(count=len(file_list), \n",
    "                    compress='lzw', \n",
    "                    crs=rio.crs.CRS({\"init\": \"epsg:3005\"})) # BC Albers equal area geographic projection\n",
    "\n",
    "    with rio.open(siteprod_tif_path, 'w', **meta) as dst:\n",
    "        print('\\nStacking siteprod raster data into a single multiband GeoTIFF file...')\n",
    "        for id, layer in enumerate(file_list, start=1):\n",
    "            print('... processing species', siteprod_layerspecies[id-1])\n",
    "            with rio.open(layer) as src:\n",
    "                dst.write_band(id, src.read(1))\n",
    "            os.remove(layer) # delete intermediate GeoTIFF (not needed anymore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a species code lookup dict to map VRI species codes to one of the 22 siteprod species codes. We built the key list for this dict by compiling unique values from the `SPECIES_CD_1` column in the VRI feature dataset, and netting out the 22 siteprod species codes. Assignments with question mark comments may need to be revisited. If this project is ported to a different landbase, this list would likely need to be expanded (we can eventually just build a BC-wide species code map from the unfiltered VRI dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siteprod_species_lookup(s):\n",
    "    spp = {'AC': 'AT',\n",
    "           'PLI': 'PL',\n",
    "           'FDI': 'FD',\n",
    "           'S': 'SW',\n",
    "           'SXL': 'SX',\n",
    "           'ACT': 'AT',\n",
    "           'E': 'EP',\n",
    "           'P': 'PL',\n",
    "           'EA': 'EP',\n",
    "           'SXW': 'SX',\n",
    "           'W': 'EP', # ?\n",
    "           'T': 'LT', # ?\n",
    "           'L': 'LT',\n",
    "           'B': 'BL',\n",
    "           'ACB': 'AT',\n",
    "           'PJ': 'PL', # ?\n",
    "           'WS': 'EP',\n",
    "           'LA':'LT',\n",
    "           'AX':'AT',\n",
    "           'BB':'BL',\n",
    "           'H':'HW', \n",
    "           'BM':'BL',\n",
    "           'V':'DR',\n",
    "           'F':'FD',\n",
    "           'C':'CW',\n",
    "           'XC':'PL',\n",
    "           'XD':'SW',\n",
    "           'X':'SW',\n",
    "           'A':'AT',\n",
    "           'D':'DR',\n",
    "           'Z':'SW',\n",
    "           'Q':'AT',\n",
    "           'Y':'YC',\n",
    "           'R':'DR',\n",
    "           'G':'DR'} # ?\n",
    "    try:\n",
    "        result = spp[s]\n",
    "    except:\n",
    "        try:\n",
    "            result = spp[s[0]]\n",
    "        except:\n",
    "            print(s)\n",
    "            assert False # bad species code\n",
    "    return result\n",
    "    \n",
    "species_list = list(set().union(*[ria_vri_vclr1p['SPECIES_CD_%i' % i].unique() for i in range(1, 7)]))\n",
    "species_list = [s for s in species_list if s is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and filter VRI feature dataset. This includes replacing null data values in certain columns with specific non-null values ('X' or 0, depending on field type) to ensure that data processing logic works correctly in downstream steps. \n",
    "\n",
    "In earlier versions of this code we cast some columns to `category` data type (in an attempt to reduce memory footprint and maybe speeds up some processing), but this proved to be more trouble than it was worth and that code is commented out in the current version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each feature in the VRI dataaset, extract mean SI data from the pixels in the siteprod raster layer corresponding the leading species. This step is computationally intensive (takes about 30 minutes to run), so this is a good time to go for a walk.\n",
    "\n",
    "Cache a copy of result to a feather file (can later be imported to speed up processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_checkpoint2 = 1\n",
    "if process_checkpoint2:\n",
    "    for i in range(1, 7):\n",
    "        f['SPECIES_CD_%i' % i].fillna('X', inplace=True)\n",
    "        f['SPECIES_PCT_%i' % i].fillna(0, inplace=True)\n",
    "    f['SOIL_NUTRIENT_REGIME'].fillna('X', inplace=True)\n",
    "    f['SOIL_MOISTURE_REGIME_1'].fillna('X', inplace=True)\n",
    "    f['SITE_POSITION_MESO'].fillna('X', inplace=True)\n",
    "    f['BCLCS_LEVEL_3'].fillna('X', inplace=True)\n",
    "    f['BCLCS_LEVEL_4'].fillna('X', inplace=True)\n",
    "    f['BCLCS_LEVEL_5'].fillna('X', inplace=True)\n",
    "    f['BEC_VARIANT'].fillna('X', inplace=True)\n",
    "    f['LIVE_STAND_VOLUME_125'].fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP1_125.fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP2_125.fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP3_125.fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP4_125.fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP5_125.fillna(0, inplace=True)\n",
    "    f.LIVE_VOL_PER_HA_SPP6_125.fillna(0, inplace=True)\n",
    "    f = f[f.BCLCS_LEVEL_2 == 'T'] # implies f.BCLCS_LEVEL_1 == 'V'\n",
    "    #f = f[f.BCLCS_LEVEL_5 != 'OP']\n",
    "    f = f[f.NON_PRODUCTIVE_CD != None]\n",
    "    f = f[f.FOR_MGMT_LAND_BASE_IND == 'Y']\n",
    "    f = f[~f.BEC_ZONE_CODE.isin(['BAFA', 'IMA'])]\n",
    "    f = f[f.PROJ_AGE_1 >= 30]\n",
    "    f = f[f.BASAL_AREA >= 5]\n",
    "    f = f[f.LIVE_STAND_VOLUME_125 >= 1]\n",
    "    f.shape\n",
    "    #vri_vclr1p_categorical_columns = open(vri_vclr1p_categorical_columns_path).read().split('\\n')\n",
    "    #for c in vri_vclr1p_categorical_columns: \n",
    "    #    f[c] = f[c].astype('category')\n",
    "    with rio.open(siteprod_tif_path) as src:\n",
    "        def mean_siteprod(r):\n",
    "            a, _ = mask(src, r.geometry, crop=True)\n",
    "            s = r.SPECIES_CD_1 \n",
    "            s = s if s in siteprod_specieslayer else siteprod_species_lookup(s)\n",
    "            i = siteprod_specieslayer[s]\n",
    "            aa = a[i]\n",
    "            return np.mean(aa[aa > 0])\n",
    "        f['siteprod'] = f.swifter.apply(mean_siteprod, axis=1)\n",
    "    f.to_feather(ria_vri_vclr1p_checkpoint2_feather_path)\n",
    "else:\n",
    "    print('loading VRI data from checkpoint2 feather')\n",
    "    f = gpd.read_feather(ria_vri_vclr1p_checkpoint2_feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VRI data includes merchantable growing stock (per ha) and species codes for the top 7 species in each stand. We recompile the volume data into species-wise columns (using species codes from `species_list`, defined earlier). This makes the table bigger (lots of null values, as only up to 7 species volume columns will have data), but makes the processing logic simpler and more stable later on.\n",
    "\n",
    "Cache a copy of result to a feather file (can later be imported to speed up processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_checkpoint3 = 1\n",
    "if process_checkpoint3:\n",
    "    def compile_species_vol(df, species):\n",
    "        import swifter\n",
    "        return df.swifter.apply(lambda r: sum(r['LIVE_VOL_PER_HA_SPP%i_125' % i] \n",
    "                                              for i in range(1, 7)\n",
    "                                              if r['SPECIES_CD_%i' % i] == species), axis=1)\n",
    "    cols = list(itertools.chain.from_iterable(['LIVE_VOL_PER_HA_SPP%i_125' % i]+['SPECIES_CD_%i' % i] for i in range(1, 7)))    \n",
    "    f_ = f[cols]\n",
    "    result = lbview.map_async(compile_species_vol, \n",
    "                              [f_]*len(species_list), \n",
    "                              species_list, \n",
    "                              ordered=True)\n",
    "    rc.wait_interactive()\n",
    "    \n",
    "    for i, species in enumerate(species_list):\n",
    "        print('compiling species', species)\n",
    "        f['live_vol_per_ha_125_%s' % species] = result[i]\n",
    "    f.to_feather(ria_vri_vclr1p_checkpoint3_feather_path)\n",
    "else:\n",
    "    print('loading VRI data from checkpoint3 feather')\n",
    "    f = gpd.read_feather(ria_vri_vclr1p_checkpoint3_feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some utility functions that we will use later to classify VRI records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_conif(species_code):\n",
    "    # return True if species_code is coniferous species\n",
    "    return species_code[:1] in ['B', 'C', 'F', 'H', 'J', 'L', 'P', 'S', 'T', 'Y']\n",
    "\n",
    "\n",
    "def is_decid(species_code):\n",
    "    # return True if species_code is deciduous species\n",
    "    return species_code[:1] in ['A', 'D', 'E', 'G', 'M', 'Q', 'R', 'U', 'V', 'W']\n",
    "\n",
    "\n",
    "def pconif(r):\n",
    "    # return proportion of volume from coniferous species in record r\n",
    "    return sum(r['SPECIES_PCT_%i' % i] for i in range(1, 7) if is_conif(r['SPECIES_CD_%i' % i])) / 100.\n",
    "\n",
    "\n",
    "def pdecid(r):\n",
    "    # return proportion of volume from deciduous species in record r\n",
    "    return sum(r['SPECIES_PCT_%i' % i] for i in range(1, 7) if is_decid(r['SPECIES_CD_%i' % i])) / 100.\n",
    "    \n",
    "\n",
    "def classify_stand_cdm(r):\n",
    "    # Classify stand (from VRI record r) as one of: conif (c), decid (d), or mixed (m), where\n",
    "    #   c >= 80% softwood\n",
    "    #   d >= 80% hardwood\n",
    "    #   m otherwise\n",
    "    if pconif(r) >= 0.8:\n",
    "        return 'c'\n",
    "    elif pdecid(r) >= 0.8:\n",
    "        return 'd'\n",
    "    else:\n",
    "        return 'm'\n",
    "    \n",
    "def classify_stand_forest_type(r):\n",
    "    # to (approximately) match TSA 41 TSR data package AU regeneration logic\n",
    "    if pconif(r) >= 0.75:\n",
    "        return 1 # pure conif\n",
    "    elif pconif(r) >= 0.50:\n",
    "        return 2 # conif mix\n",
    "    elif pconif(r) >= 0.25:\n",
    "        return 3 # decid mix\n",
    "    else:\n",
    "        return 4 # pure decid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to generate stratum codes for a given VRI record `r`. \n",
    "\n",
    "In standard (not lexmatch) mode, this function will generate a stratum code string including BEC zone code, leading species code, and secondary species code (if classified as a mixedwood stand according to BCLCS codes). \n",
    "\n",
    "In lexmatch mode, this will generate a longer stratum code string intended for lexicographic matching (used later with a clustering algorithm that finds \"best match\" strata based on these codes. Stratum code strings are built from the same data, but use special \"lexmatch\" versions of the data fields where data values have enhanced. Lexmatch field values are recompiled to a fixed width (padded with `x` character), and in the case of (primary, secondary, tertiary) species code fields the first charater (representing genus, sort of) is duplicated. These lexmatch field values are repeated (3 times for the BEC zone code, 2 times for the primary species code) within the lexmatch stratum codes to influence the relative importance of different components during matching. \n",
    "\n",
    "Note that we tested several alternative stratum code formulations before settling on this one, which yielded consistently good results across all 5 TSAs in the study area. In searching for a \"good\" stratification scheme, we were looking for relatively tight SI distribution within each stratum (indicating that the aggregated sites have a similar growth potential) and a high proportion of total area coverage in the top 15 or so strata within a given TSA (indicating that we can compile growth and yield curves for a relatively small number of strata while still covering most of the forest).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_stand(r, lexmatch=False, lexmatch_fieldname_suffix='_lexmatch'):\n",
    "    result = ''\n",
    "    if lexmatch:\n",
    "        result += 3 * r['BEC_ZONE_CODE%s' % lexmatch_fieldname_suffix]\n",
    "        result += '_'\n",
    "        #result += r.BCLCS_LEVEL_5\n",
    "        #result += '_'\n",
    "        result += 2 * r['SPECIES_CD_1%s' % lexmatch_fieldname_suffix]\n",
    "        if r.BCLCS_LEVEL_4 == 'TM' and r.SPECIES_CD_2 != None:\n",
    "            result += '+' + r['SPECIES_CD_2%s' % lexmatch_fieldname_suffix]\n",
    "    else:\n",
    "        result += r.BEC_ZONE_CODE\n",
    "        result += '_'\n",
    "        #result += r.BCLCS_LEVEL_5\n",
    "        #result += '_'\n",
    "        result += r.SPECIES_CD_1\n",
    "        if r.BCLCS_LEVEL_4 == 'TM' and r.SPECIES_CD_2 != None:\n",
    "            result += '+' + r.SPECIES_CD_2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile enhanced lexmatch fields as described above, and use the `stratify_stands` function to compile `stratum` and `stratum_lexmatch` columns.  We use the `swifter` version of the `apply` method here, which will attempt to automatically parallelize processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['BEC_ZONE_CODE_lexmatch'] = f.BEC_ZONE_CODE.str.ljust(4, fillchar='x')\n",
    "for i in range(1, 3):\n",
    "    f['SPECIES_CD_%i_lexmatch' % i] = f['SPECIES_CD_%i' % i].str.ljust(4, 'x')\n",
    "    f['SPECIES_CD_%i_lexmatch' % i] = f['SPECIES_CD_%i' % i].str[:1] + f['SPECIES_CD_%i' % i]\n",
    "\n",
    "stratify_stand = stratify_stand\n",
    "stratify_stand_lexmatch = partial(stratify_stand, lexmatch=True)\n",
    "\n",
    "f['stratum'] = f.swifter.apply(stratify_stand, axis=1)\n",
    "f['stratum_lexmatch'] = f.swifter.apply(stratify_stand_lexmatch, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_col = 'stratum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile `forest_type` column (1: softwood, 2: softwood mix, 3: hardwood mix, 4: hardwood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f['forest_type'] = f.reset_index().swifter.apply(classify_stand_forest_type, axis=1)\n",
    "f['forest_type'] = f.apply(classify_stand_forest_type, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint 4 stand dataframe to feather file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_feather(ria_vri_vclr1p_checkpoint4_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: # roll back if screwed up further down\n",
    "    f = gpd.read_feather(ria_vri_vclr1p_checkpoint4_feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with inventory data pre-processing now. In the next part of the procedure, we define strata and analysis units (AUs) within each TSA, and generate VDYP and TIPSY yield curves for each AU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile strata, AUs, and yield curves for each TSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = f.reset_index().set_index('tsa_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some empty dicts to store various outputs from the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdyp_curves_smooth = {}\n",
    "vdyp_results = {}\n",
    "tipsy_params = {}\n",
    "tipsy_curves = {}\n",
    "scsi_au = {}\n",
    "au_scsi = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    def process_vdyp_out(vdyp_out, volume_flavour='Vdwb', min_age=30, max_age=300, \n",
    "                         sigma_c1=10, sigma_c2=0.4, dx_c1=0.5, dx_c2=10,\n",
    "                         window=10,  skip1=0, skip2=30, maxfev=100000,\n",
    "                         body_fit_func=body_fit_func, body_fit_func_bounds_func=body_fit_func_bounds_func,\n",
    "                         toe_fit_func=toe_fit_func, toe_fit_func_bounds_func=toe_fit_func_bounds_func):\n",
    "        vdyp_out_concat = pd.concat([v for v in vdyp_out.values() if type(v) == pd.core.frame.DataFrame])\n",
    "        c = vdyp_out_concat.groupby(level='Age')[volume_flavour].median()\n",
    "        c = c[c > 0]\n",
    "        c = c[c.index >= min_age]\n",
    "        x = c.index.values\n",
    "        y = c.rolling(window=window, center=True).median().values\n",
    "        x, y = x[y > 0], y[y > 0]\n",
    "        x, y = x[skip1:], y[skip1:]\n",
    "        #return x, y\n",
    "        y_mai = pd.Series(y / x, x)\n",
    "        y_mai_max_age = y_mai.idxmax()\n",
    "        sigma = (np.abs(x - y_mai_max_age) + sigma_c1)**sigma_c2\n",
    "        popt, pcov = curve_fit(body_fit_func, x, y, bounds=body_fit_func_bounds_func(x), maxfev=maxfev, sigma=sigma)\n",
    "        x = np.array(range(1, max_age))\n",
    "        y = fit_func1(x, *popt)\n",
    "        dx = max(0, dx_c1 * popt[2] - dx_c2)\n",
    "        print(dx, dx_c1, popt[2], dx_c2)\n",
    "        x, y, (i1, popt_toe) = fill_curve_left(x, y, skip=skip2, dx=dx, maxfev=maxfev,\n",
    "                                               toe_fit_func=toe_fit_func, toe_fit_func_bounds_func=toe_fit_func_bounds_func)\n",
    "        print(popt_toe)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some keyword argument override values, to fine-tune curve generation for a few difficult AUs. Most AUs get processed without problems using default parameters. We found these special cases after visually inspecting output curves using default parameters, and determined the custom parameters values through \"expert tuning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwarg_overrides = {'08':{('BWBS_SB', 'H'):{'skip1':30},\n",
    "                         ('BWBS_S', 'L'):{'skip1':50},\n",
    "                         ('SWB_S', 'L'):{'skip1':30},\n",
    "                         ('BWBS_AT', 'H'):{'skip1':30}},\n",
    "                   '16':{('SWB_SX', 'L'):{'skip1':30}},\n",
    "                   '24':{('ESSF_BL', 'L'):{'skip1':30}},\n",
    "                   '40':{('BWBS_SX', 'L'):{'skip1':30},\n",
    "                         ('SWB_SX', 'L'):{'skip1':60, 'dx_c1':1., 'dx_c2':0.}},\n",
    "                   '41':{('ESSF_BL', 'L'):{'skip1':60},\n",
    "                         ('ESSF_SE', 'M'):{'skip1':30}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tsa = '41'\n",
    "    vdyp_curves_smooth_tsa_feather_path = '%s%s.feather' % (vdyp_curves_smooth_tsa_feather_path_prefix, tsa)\n",
    "    #if not os.path.isfile(vdyp_curves_smooth_tsa_feather_path):\n",
    "    if 1:\n",
    "        figsize = (8, 6)\n",
    "        plot = 1\n",
    "        vdyp_smoothxy = {}\n",
    "        palette_flavours=['RdPu', 'Blues', 'Greens', 'Greys']\n",
    "        palette = sns.color_palette('Greens', 3)\n",
    "        sns.set_palette(palette)\n",
    "        alphas = [1.0, 0.5, 0.1]\n",
    "        for stratumi, sc, result in results[tsa]:\n",
    "            if sc != 'ESSF_SE': continue\n",
    "            if plot: fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "            print('stratum',stratumi, sc)\n",
    "            #for i, si_level in enumerate(si_levels):\n",
    "            for i, si_level in enumerate(['M']):            \n",
    "                print('processing', sc, si_level)\n",
    "                vdyp_out = vdyp_results[tsa][stratumi][si_level]\n",
    "                kwargs = {}\n",
    "                if (sc, si_level) in kwarg_overrides[tsa]:\n",
    "                    kwargs.update(kwarg_overrides[tsa][(sc, si_level)])\n",
    "                x, y = process_vdyp_out(vdyp_out, **kwargs)\n",
    "                df = pd.DataFrame(zip(x, y), columns=['age', 'volume'])\n",
    "                df = df[df.volume > 0]\n",
    "                df['stratum_code'] = sc\n",
    "                df['si_level'] = si_level\n",
    "                vdyp_smoothxy[(sc, si_level)] = df \n",
    "                if plot:\n",
    "                    vdyp_out_concat = pd.concat([v for v in vdyp_out.values() if type(v) == pd.core.frame.DataFrame])\n",
    "                    c = vdyp_out_concat.groupby(level='Age')['Vdwb'].median()\n",
    "                    c = c[c > 0]\n",
    "                    c = c[c.index >= 30]\n",
    "                    x_ = c.index.values\n",
    "                    y_ = c.values\n",
    "                    plt.plot(x_, y_, linestyle=':', label='VDYP->agg (%s %s)' % (sc, si_level), \n",
    "                             linewidth=2, color=palette[i])\n",
    "                    plt.plot(x, y, label='%s %s' % (sc, si_level))\n",
    "            if plot: \n",
    "                plt.legend()\n",
    "                plt.xlim([0, 300])\n",
    "                plt.ylim([0, 600])\n",
    "                plt.tight_layout()\n",
    "        vdyp_curves_smooth[tsa] = pd.concat(vdyp_smoothxy.values()).reset_index()#.set_index(['stratum_code', 'si_level'])\n",
    "        vdyp_curves_smooth[tsa].to_feather(vdyp_curves_smooth_tsa_feather_path)\n",
    "    #else:\n",
    "    #    vdyp_curves_smooth[tsa] = pd.read_feather(vdyp_curves_smooth_tsa_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    vdyp_out_cache = pickle.load(open('vdyp_out_cache.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    force_run_vdyp = 0\n",
    "    #tsa = '08' # fix bwbs sb h\n",
    "    tsa = '16'\n",
    "    #tsa = '24'\n",
    "    #tsa = '40' # fix bwbs sx l\n",
    "    #tsa = '41' \n",
    "    stratum_col = 'stratum'\n",
    "    %run 01a_run-tsa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over TSAs and run notebook `01a_run-tsa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.set_index('tsa_code', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over TSAs and run the child notebook. This will compile strata and AUs, run VDYP on each AU and compile yield curves, and generate TIPSY input files. \n",
    "\n",
    "VDYP runs individually on each stand in each AU, and we post-process output by age class. This geneally produces \"jittery\" yield curves (jitterier for AUs with fewer stands), which is not great for use as input to wood supply models. So, we run a nonlinear parameter fitting process on aggregated output to produce a \"smooth\" curve (to find a best-fit curve using a flexible curve function we developed). We make sure that the smooth curve is a good match for the original jittery curve around CMAI (i.e., culmination of MAI curve, or optimal rotation age), however this sometimes leads to minor distortions in older age classes (no big deal, and the fit seems to be really good or dominant AUs). \n",
    "\n",
    "The inventory data is left-censored (because there is no merchantable voluem in the filtered dataset for the first few age classes), so we need to run a second nonlinear parameter fitting process to generate a nice \"ramp\" that has a null slope on the far left (at age 0) and matches the slope of first few data points on the rest of the curve. \n",
    "\n",
    "See child notebook for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdyp_out_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 1:\n",
    "    force_run_vdyp = 0\n",
    "    for tsa in ria_tsas[:]:\n",
    "        stratum_col = 'stratum'\n",
    "        %run 01a_run-tsa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    pickle.dump(vdyp_out_cache, open('vdyp_out_cache.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause running notebook and head to a Windows machine to run `02_input-tsa*.dat` files through BatchTIPSY, then copy `04_output-tsa*.out` files to `./data/`.\n",
    "\n",
    "We tried to get TIPSY to run on a linux VM under Wine, but there is no way to run the software other than clicking through the GUI (yuck). We still might be able to get TIPSY fully automated, but this would require some sort of robot mouse click automation that we have not had time to figure out yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over TSAs and run notebook `01b_run-tsa`. This compiles output data from TIPSY, and plots results on top of smoothed VDYP output (for visual inspection of AU-wise unmanaged+managed curve pairs, to confirm that projected yields are reasonable before proceeding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over tsas here and run notebook 01_run-tsa_step2\n",
    "for tsa in ria_tsas[:]:\n",
    "    %run 01b_run-tsa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export curves to CSV data tables (for soft-link to `spadesCBM`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed attempt to automate translating VRI species codes to CANFI species codes. Could potentially work later, if the `LandR_sppEquivalencies.csv` data table was set up correctly to allow this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    spp_map = pd.read_csv('./data/LandR_sppEquivalencies.csv')\n",
    "    spp_map = spp_map[(~spp_map['LANDIS_traits'].isnull()) & (~spp_map['BC_Forestry'].isnull())][['BC_Forestry', 'LANDIS_traits']]\n",
    "    spp_map['vri'] = spp_map['BC_Forestry'].str.upper()\n",
    "    spp_map['link'] = spp_map['LANDIS_traits']\n",
    "    canfi = pd.read_csv('./data/canfi_species.csv')\n",
    "    canfi['link'] = canfi.genus + '.' + canfi.species\n",
    "    canfi = canfi.set_index('link').merge(spp_map[['link', 'vri']], on='link', how='left')\n",
    "    canfi = canfi[~canfi.vri.isnull()].set_index('vri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dict mapping VRI species codes to CANFI species codes (`spadesCBM` uses CANFI species codes), and a function that returns CANFI code for leading species given a stratum code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canfi_map = {'AC':1211, \n",
    "             'AT':1201, \n",
    "             'BL':304, \n",
    "             'EP':1303, \n",
    "             'FDI':500, \n",
    "             'HW':402, \n",
    "             'PL':204, \n",
    "             'PLI':204, \n",
    "             'SB':101, \n",
    "             'SE':104, \n",
    "             'SW':105, \n",
    "             'SX':100,\n",
    "             'S':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canfi_species(stratum_code):\n",
    "    s = stratum_code.split('_')[-1].split('+')[0]\n",
    "    result = canfi_map[s]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some empty data structures to store output, and compile three data tables that will be used to import yield curve data into `spadesCBM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original section of convert yield curve to ws3 inputs\n",
    "au_table_data = {'au_id':[],\n",
    "                 'tsa':[],\n",
    "                 'stratum_code':[],\n",
    "                 'si_level':[],\n",
    "                 'canfi_species':[],\n",
    "                 'unmanaged_curve_id':[],\n",
    "                 'managed_curve_id':[]} \n",
    "\n",
    "curve_table_data = {'curve_id':[],\n",
    "                    'curve_type':[]}\n",
    "\n",
    "curve_points_table_data = {'curve_id':[],\n",
    "                           'x':[],\n",
    "                           'y':[]}\n",
    "\n",
    "for tsa in ria_tsas:\n",
    "    print(tsa)\n",
    "    vdyp_curves_ = vdyp_curves_smooth[tsa].set_index(['stratum_code', 'si_level'])\n",
    "    tipsy_curves_ = tipsy_curves[tsa].reset_index().set_index('AU')\n",
    "    for stratum_code, si_level in list(vdyp_curves_.index.unique()):\n",
    "        for i, si_level in enumerate(si_levels, start=1):\n",
    "            au = 1000*i + stratumi\n",
    "            scsi_au[tsa][(sc, si_level)] = au\n",
    "        au_id_ = scsi_au[tsa][(stratum_code, si_level)]\n",
    "        tipsy_curve_id = 20000 + au_id_\n",
    "        is_managed_au = tipsy_curve_id in tipsy_curves_.index.unique()\n",
    "        au_id = 100000 * int(tsa) + au_id_\n",
    "        unmanaged_curve_id = au_id\n",
    "        managed_curve_id = au_id + 20000 if is_managed_au else unmanaged_curve_id\n",
    "        #print(au_id, stratum_code, si_level, is_managed_au, unmanaged_curve_id, managed_curve_id)\n",
    "        au_table_data['au_id'].append(au_id)\n",
    "        au_table_data['tsa'].append(tsa)\n",
    "        au_table_data['stratum_code'].append(stratum_code)\n",
    "        au_table_data['si_level'].append(si_level)\n",
    "        au_table_data['canfi_species'].append(canfi_species(stratum_code))\n",
    "        au_table_data['unmanaged_curve_id'].append(unmanaged_curve_id)\n",
    "        curve_table_data['curve_id'].append(unmanaged_curve_id)\n",
    "        curve_table_data['curve_type'].append('unmanaged')\n",
    "        vdyp_curve = vdyp_curves_.loc[(stratum_code, si_level)]\n",
    "        #print('vdyp curve')\n",
    "        for x, y in zip(vdyp_curve.age, vdyp_curve.volume):\n",
    "            #print(x, round(y, 2))\n",
    "            curve_points_table_data['curve_id'].append(unmanaged_curve_id)\n",
    "            curve_points_table_data['x'].append(int(x))\n",
    "            curve_points_table_data['y'].append(round(y, 2))\n",
    "        au_table_data['managed_curve_id'].append(managed_curve_id)\n",
    "        if is_managed_au:\n",
    "            curve_table_data['curve_id'].append(managed_curve_id)\n",
    "            curve_table_data['curve_type'].append('managed')        \n",
    "            tipsy_curve = tipsy_curves_.loc[tipsy_curve_id]\n",
    "            #print('tipsy curve')\n",
    "            for x, y in zip(tipsy_curve.Age, tipsy_curve.Yield):\n",
    "                #print(x, round(y, 2))\n",
    "                curve_points_table_data['curve_id'].append(managed_curve_id)\n",
    "                curve_points_table_data['x'].append(int(x))\n",
    "                curve_points_table_data['y'].append(round(y, 2))\n",
    "au_table = pd.DataFrame(au_table_data)\n",
    "curve_table = pd.DataFrame(curve_table_data)\n",
    "curve_points_table = pd.DataFrame(curve_points_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few lines of each table (quick sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_points_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export tables to CSV for soft-link with `spadesCBM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_table.to_csv('./data/spadescbm_bundle/au_table.csv')\n",
    "curve_table.to_csv('./data/spadescbm_bundle/curve_table.csv')\n",
    "curve_points_table.to_csv('./data/spadescbm_bundle/curve_points_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute THLB status to VRI stands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section we impute timber harvesting landbase (THLB) status to the VRI stands. \n",
    "\n",
    "The process may seem very complex (because it is), however this is to _only_ (therefore simplest) procedure we were able to develop that comes close to matching TSA-wise THLB area reported in timber supply review (TSR) documentation published on the BC government web site.\n",
    "\n",
    "If we had access to the original inventory datasets used in the TSR process, we could potentially just lift the THLB attribute from that. Alternatively, we could potentially rebuild the THLB \"from the ground up\" for each TSA by attempting to replicate THLB netdown documented in TSR documentation (neither trivial nor guaranteed to yield the \"correct\" THLB area).\n",
    "\n",
    "Our method may be a bit messy, but it is robust insofar as we were able to get it to work for five TSAs (albeit with quite a bit of trial-and-error parameter tuning for certain TSAs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the VRI dataset from checkpoint 1, and filter records to include only the forested features to which we want to impute an AU and managment status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gpd.read_feather(ria_vri_vclr1p_checkpoint1_feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a [THLB raster data layer]:(https://www.hectaresbc.org/app/habc/HaBC.html?type=raster&query=misc.thlb) on the [Hectares BC]:(https://hectaresbc.ca/app/habc/HaBC.html) web site (1 ha pixels). This website is maintained by the BC government (so we assume that data made available there is legitimate), however there is no metadata provided with the THLB raster data layer (so we cannot confirm the date it was compile, author, methodology, etc).\n",
    "\n",
    "The first step in the process is to calculate stand-wise mean THLB (by masking the THLB raster data layer with stand polygons to extract the relevant pixels, and calculating mean value of all pixels with non-null values), and store the result in the `thlb_raw` field in the inventory dataframe. Note that THLB raster pixels.\n",
    "\n",
    "Note that non-null data values in the THLB raster layer are in the range $[0, 100]$, which we are assuming as \"percent attribution to THLB status\". THLB should logically be a binary variable (not a percent), seeing as stands either are or are not eligible for harvesting. Perhaps the raster data is recompiled from a binary THLB attribute in a high-resolution polygon data layer with small sub-pixel areas netted out of THLB for various reasons (impossible to say from limited metadata). In later steps, we need to coerce this into a stand-wise binary THLB attribute to be compatible with `spades_ws3` module downstream.\n",
    "\n",
    "Extract from THLB raster layer metadata follows, in case this helps track down some of the missing answers later.\n",
    "\n",
    "<blockquote>\n",
    "    \n",
    "Description\n",
    "The timber harvesting land base (THLB) is defined for timber supply analysis. The THLB is Crown forest land where timber harvesting is considered acceptable and economically feasible given objectives for all relevant forest values, existing timber quality, market values and available technology. It approximates the areas on which timber harvesting can reasonably be expected or extrapolated given existing forest management objectives and practices. The THLB is not meant to be a legally-defined or geographically static entity.\n",
    "\n",
    "Creator / Publisher\n",
    "Ministry of Forests and Range, Forest Analysis and Inventory Branch\n",
    "\n",
    "Contact\n",
    "Doug Layden (Doug.Layden@gov.bc.ca, Timber Supply Analyst, Ministry of Forests and Range, Forest Analysis and Inventory Branch\n",
    "\n",
    "Load Date\n",
    "April 13, 2011\n",
    "\n",
    "Coverage\n",
    "Timber Supply Areas. Not Tree Farm Licences.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open('./data/misc.thlb.tif') as src:\n",
    "    def mean_thlb(r):\n",
    "        try:\n",
    "            a, _ = mask(src, r.geometry, crop=True)\n",
    "        except:\n",
    "            return 0\n",
    "        return np.mean(a[a >= 0])\n",
    "    f['thlb_raw'] = f.swifter.apply(mean_thlb, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from an earlier (failed) attempt to impute a binary THLB attribute to VRI data directly from a data layer from a previous (PICS BC forest carbon) project. No dice (runs, but resulting TSA-wise THLB area does not match THLB area reported in TSR documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    thlb_rasterprop_thresh = 0.01\n",
    "    with rio.open('./data/ria_demo/ria_landscapestack_init.tif') as src:\n",
    "        def is_thlb(r):\n",
    "            try:\n",
    "                a, _ = mask(src, r.geometry, crop=True)\n",
    "            except:\n",
    "                return 0\n",
    "            aa = a[1]\n",
    "            return 1 if np.mean(aa[aa >= 0]) > thlb_rasterprop_thresh else 0\n",
    "        f['thlb'] = f.swifter.apply(is_thlb, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter VRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    f = f[f.BCLCS_LEVEL_2 == 'T'] # implies f.BCLCS_LEVEL_1 == 'V'\n",
    "    #f = f[f.NON_PRODUCTIVE_CD != None]\n",
    "    f = f[f.FOR_MGMT_LAND_BASE_IND == 'Y']\n",
    "    f = f[~f.BEC_ZONE_CODE.isin(['BAFA', 'IMA'])]\n",
    "    f = f[~f.SPECIES_CD_1.isnull()]\n",
    "    f = f[~f.BCLCS_LEVEL_5.isnull()]\n",
    "    f = f[~f.SITE_INDEX.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check row and column count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile standard and lexmatch stratum fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['BEC_ZONE_CODE_lexmatch'] = f.BEC_ZONE_CODE.str.ljust(4, fillchar='x')\n",
    "for i in range(1, 3):\n",
    "    f['SPECIES_CD_%i_lexmatch' % i] = f['SPECIES_CD_%i' % i].str.ljust(4, 'x')\n",
    "    f['SPECIES_CD_%i_lexmatch' % i] = f['SPECIES_CD_%i' % i].str[:1] + f['SPECIES_CD_%i' % i]\n",
    "\n",
    "stratify_stand = stratify_stand\n",
    "stratify_stand_lexmatch = partial(stratify_stand, lexmatch=True)\n",
    "\n",
    "f['stratum'] = f.swifter.apply(stratify_stand, axis=1)\n",
    "f['stratum_lexmatch'] = f.swifter.apply(stratify_stand_lexmatch, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_feather(ria_vri_vclr1p_checkpoint5_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    f = pd.read_feather(ria_vri_vclr1p_checkpoint5_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_col = 'stratum'\n",
    "f['%s_matched' % stratum_col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    au_table = pd.read_csv('./data/spadescbm_bundle/au_table.csv')\n",
    "    curve_table = pd.read_csv('./data/spadescbm_bundle/curve_table.csv')\n",
    "    curve_points_table = pd.read_csv('./data/spadescbm_bundle/curve_points_table.csv')\n",
    "    au_table['tsa'] = au_table.apply(lambda r: '%02d' % r.tsa, axis=1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile `stratum_matched` column.\n",
    "\n",
    "For stands in one of the compiled strata in each TSA (and filtered records that happen to have a stratum code in the list of compiled strata), we just use that stratum code (easy peasy). For the remaining records, we use the \"best match\" stratum code (which we derive from a lexicographic analysis that defines best match based on minimum Levenstein distance between the \"lexmatch\" versions of the stratum codes).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsa in ria_tsas:    \n",
    "    print('matching tsa', tsa)\n",
    "    try:\n",
    "        f.reset_index(inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    stratum_codes = list(au_table.set_index('tsa').loc[tsa].stratum_code.unique())\n",
    "    f_ = f.set_index('tsa_code').loc[tsa].set_index('stratum')\n",
    "    totalarea = f_.FEATURE_AREA_SQM.sum()\n",
    "    f_['totalarea_p'] = f_.FEATURE_AREA_SQM / totalarea\n",
    "    names1 = set(f_.loc[stratum_codes].stratum_lexmatch.unique())\n",
    "    names2 = set(f_.stratum_lexmatch.unique()) - names1\n",
    "    stratum_key = f_.reset_index().groupby('%s_lexmatch' % stratum_col)[stratum_col].first()\n",
    "    totalarea_p_sum__ = f_.groupby('%s_lexmatch' % stratum_col).totalarea_p.sum()\n",
    "    lev_dist = {n2:{n1:distance.levenshtein(n1, n2) for n1 in names1} for n2 in names2}\n",
    "    lev_dist_low = {n2:{n1:(lev_dist[n2][n1], totalarea_p_sum__.loc[n1]) \n",
    "                        for n1 in lev_dist[n2].keys() if lev_dist[n2][n1] == min(lev_dist[n2].values())} \n",
    "                    for n2 in names2}\n",
    "    bm = {stratum_key.loc[n2]:stratum_key[max(lev_dist_low[n2].items(), key=operator.itemgetter(1))[0]] for n2 in names2}\n",
    "    f_.reset_index(inplace=True)\n",
    "    c, sc = stratum_col, stratum_codes\n",
    "    f_['%s_matched' % stratum_col] = f_.swifter.apply(lambda r: r[c] if r[c] in sc else bm[r[c]], axis=1)\n",
    "    \n",
    "    m = f[['FEATURE_ID']].merge(f_[['FEATURE_ID', 'stratum_matched']], on='FEATURE_ID', how='left').set_index('FEATURE_ID')\n",
    "    f.set_index('FEATURE_ID', inplace=True)\n",
    "    f['stratum_matched'] = m.stratum_matched.where(~m.stratum_matched.isnull(), \n",
    "                                                   f.stratum_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a dataframe of stratum-wise site index (SI) statistics, and then use this to assign `si_level` (H, M, L) attribute to each stand, based on stratum-level SI quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_si_stats = f.groupby('stratum_matched').SITE_INDEX.describe(percentiles=[0, 0.05, 0.20, 0.35, 0.5, 0.65, 0.80, 0.95, 1])\n",
    "#si_levelquants={'L':[5, 20, 35], 'M':[35, 50, 65], 'H':[65, 80, 95]}\n",
    "si_levelquants={'L':[0, 20, 35], 'M':[35, 50, 65], 'H':[65, 80, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stratum_code in stratum_si_stats.index:\n",
    "    print(stratum_code)\n",
    "    for i, (si_level, Q) in enumerate(si_levelquants.items()):        \n",
    "        si_lo = stratum_si_stats.loc[stratum_code].loc['%i%%' % Q[0]]\n",
    "        si_md = stratum_si_stats.loc[stratum_code].loc['%i%%' % Q[1]]\n",
    "        si_hi = stratum_si_stats.loc[stratum_code].loc['%i%%' % Q[2]] \n",
    "        f.loc[(f.stratum_matched == stratum_code) & (f.SITE_INDEX >= si_lo) & (f.SITE_INDEX <= si_hi), 'si_level'] = si_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign an AU identifier code to each inventory record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def au_from_scsi(r):\n",
    "    try:\n",
    "        au_id = 100000*int(r.tsa_code) + scsi_au[r.tsa_code][(r.stratum_matched, r.si_level)]\n",
    "        return au_id\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['au'] = f.swifter.apply(au_from_scsi, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataframe shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out records with null AU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f[~f.au.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataframe shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize dataframe to feather file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_feather(ria_vri_vclr1p_checkpoint6_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    au_table.set_index('au_id', inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile two new columns assigning a yield curve ID to each inventory record. \n",
    "\n",
    "`curve1` uses managed (TIPSY) curves if applicable (i.e., if age is 60 or less and has a managed yield curve defined). \n",
    "`curve2` only uses unmanaged (VDYP) yield curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_curve1(r):\n",
    "    # age=60 managed/unmanaged cutoff assumption from Cosmin Man (personal communication)\n",
    "    au_id = 100000*int(r.tsa_code) + scsi_au[r.tsa_code][(r.stratum_matched, r.si_level)]\n",
    "    au = au_table.loc[au_id]\n",
    "    if r.PROJ_AGE_1 <= 60 and not np.isnan(au['managed_curve_id']):\n",
    "        curve_id = au['managed_curve_id']\n",
    "    else:\n",
    "        curve_id = au['unmanaged_curve_id']\n",
    "    return int(curve_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['curve1'] = f.swifter.apply(assign_curve1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_curve2(r):\n",
    "    au_id = 100000*int(r.tsa_code) + scsi_au[r.tsa_code][(r.stratum_matched, r.si_level)]\n",
    "    au = au_table.loc[au_id]\n",
    "    return au['unmanaged_curve_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['curve2'] = f.swifter.apply(assign_curve2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_feather(ria_vri_vclr1p_checkpoint7_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    f = gpd.read_feather(ria_vri_vclr1p_checkpoint7_feather_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.thlb_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.thlb_raw.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thlb_area(r):\n",
    "    if r.tsa_code == '08':\n",
    "        if r.thlb_raw < 90: \n",
    "            return 0.\n",
    "        if r.SPECIES_CD_1 in species_spruce and r.SITE_INDEX < 10:\n",
    "            return 0.\n",
    "        if r.SPECIES_CD_1 in species_pine and r.SITE_INDEX < 15:\n",
    "            return 0.\n",
    "        if r.SPECIES_CD_1 in species_aspen and r.SITE_INDEX < 15:\n",
    "            return 0.\n",
    "        if r.SPECIES_CD_1 in species_fir and r.SITE_INDEX < 10:\n",
    "            return 0.\n",
    "        if r.SPECIES_CD_1 in ('SB', 'E', 'EA', 'EB', 'LT'):\n",
    "            return 0\n",
    "    return r.thlb_raw * r.FEATURE_AREA_SQM * 0.000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['thlb_area'] = f.swifter.apply(thlb_area, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_thlb(r):\n",
    "    thlb_thresh = 50\n",
    "    if r.tsa_code == '08':\n",
    "        thlb_thresh = 93\n",
    "    elif r.tsa_code == '24':\n",
    "        thlb_thresh = 69\n",
    "    return 1 if r.thlb_raw > thlb_thresh else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['thlb'] = f.swifter.apply(assign_thlb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.query('thlb == 1').groupby('tsa_code').FEATURE_AREA_SQM.sum() * 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.groupby('tsa_code').thlb_area.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_managed_curve(r):\n",
    "    if r.thlb == 0: \n",
    "        return -1\n",
    "    else:\n",
    "        if np.isnan(au_table.loc[int(r.au)].managed_curve_id):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_feather(ria_vri_vclr1p_checkpoint8_feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    f = gpd.read_feather(ria_vri_vclr1p_checkpoint8_feather_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    f.to_file('./data/ria_vri-final.shp')\n",
    "    !zip ./data/ria_vri-final_shp.zip ./data/ria_vri-final.*\n",
    "    !mv ./data/ria_vri-final_shp.zip ./data/spadescbm_bundle/\n",
    "    !zip -r ./data/spadescbm_bundle.zip ./data/spadescbm_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_geometry(r):\n",
    "    from shapely.geometry import MultiPolygon\n",
    "    g = r.geometry\n",
    "    if not g.is_valid:\n",
    "        _g = g.buffer(0)\n",
    "        ################################\n",
    "        # HACK\n",
    "        # Something changed (maybe in fiona?) and now all GDB datasets are\n",
    "        # loading as MultiPolygon geometry type (instead of Polygon). \n",
    "        # The buffer(0) trick smashes the geometry back to Polygon, \n",
    "        # so this hack upcasts it back to MultiPolygon.\n",
    "        # \n",
    "        # Not sure how robust this is going to be (guessing not robust).\n",
    "        _g = MultiPolygon([_g])\n",
    "        assert _g.is_valid\n",
    "        assert _g.geom_type == 'MultiPolygon'\n",
    "        g = _g\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(f, tsa):\n",
    "    f_ = f[['geometry', 'tsa_code', 'thlb', 'au', 'curve1', 'curve2', 'SPECIES_CD_1', 'PROJ_AGE_1', 'FEATURE_AREA_SQM']]\n",
    "    f_ = f_.set_index('tsa_code').loc[tsa].reset_index()\n",
    "    f_.geometry = f_.swifter.apply(clean_geometry, axis=1)\n",
    "    return f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prop_names = [u'tsa_code', u'thlb', u'au', u'SPECIES_CD_1', u'PROJ_AGE_1', u'FEATURE_AREA_SQM']\n",
    "prop_names = [u'tsa_code', u'thlb', u'au', u'canfi_species', u'PROJ_AGE_1', u'FEATURE_AREA_SQM']\n",
    "prop_types = [(u'theme0', 'str:10'),\n",
    "              (u'theme1', 'str:1'),\n",
    "              (u'theme2', 'str:10'), \n",
    "              (u'theme3', 'str:5'), \n",
    "              (u'age', 'int:5'), \n",
    "              (u'area', 'float:10.1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dict(zip(prop_names, dict(prop_types).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for tsa in ria_tsas[:]:\n",
    "    print('processing tsa', tsa)\n",
    "    f_ = extract_features(f, tsa)\n",
    "    try:\n",
    "        os.mkdir('./data/shp/tsa%s.shp' % tsa)\n",
    "    except:\n",
    "        pass\n",
    "    f_.rename(columns=columns, inplace=True)\n",
    "    f_.theme0 = 'tsa' + f_.theme0\n",
    "    f_.theme2 = f_.theme2.astype(int)\n",
    "    f_.theme3 = f_.apply(lambda r: au_table.loc[r.theme2].canfi_species, axis=1)\n",
    "    f_.age = f_.age.fillna(0)\n",
    "    f_.age = f_.age.astype(int)\n",
    "    f_.area = (f_.area * 0.0001).round(1)\n",
    "    f_.to_file('./data/shp/tsa%s.shp/stands.shp' % tsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
